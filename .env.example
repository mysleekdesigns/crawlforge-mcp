# MCP WebScraper Environment Configuration
# Copy this file to .env and customize for your environment

# =============================================================================
# CORE CONFIGURATION
# =============================================================================

# Environment (development, production, staging, test)
NODE_ENV=production

# Logging configuration
LOG_LEVEL=info
SECURITY_LOGGING=true
VIOLATION_LOGGING=true
ENABLE_METRICS=true

# =============================================================================
# SEARCH PROVIDER CONFIGURATION
# =============================================================================

# Search provider: 'auto', 'google', 'duckduckgo'
# 'auto' will use Google if credentials are available, otherwise DuckDuckGo
SEARCH_PROVIDER=auto

# Google Custom Search API (optional)
# Get your API key from: https://developers.google.com/custom-search/v1/introduction
# Create a search engine at: https://cse.google.com/cse/create/new
GOOGLE_API_KEY=
GOOGLE_SEARCH_ENGINE_ID=

# DuckDuckGo Configuration
DUCKDUCKGO_TIMEOUT=30000
DUCKDUCKGO_MAX_RETRIES=3
DUCKDUCKGO_RETRY_DELAY=1000
DUCKDUCKGO_USER_AGENT=MCP-WebScraper/3.0

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================

# Worker and queue settings
MAX_WORKERS=10
QUEUE_CONCURRENCY=10

# Cache configuration
CACHE_MAX_SIZE=1000
CACHE_TTL=3600000
CACHE_ENABLE_DISK=true
CACHE_DIR=./cache

# =============================================================================
# RATE LIMITING
# =============================================================================

# Rate limiting settings
RATE_LIMIT_REQUESTS_PER_SECOND=10
RATE_LIMIT_REQUESTS_PER_MINUTE=100
RATE_LIMIT_PER_DOMAIN=true

# =============================================================================
# CRAWLING CONFIGURATION
# =============================================================================

# Crawling limits
MAX_CRAWL_DEPTH=5
MAX_PAGES_PER_CRAWL=100
RESPECT_ROBOTS_TXT=true
FOLLOW_EXTERNAL_LINKS=false

# User agent for web requests
USER_AGENT=MCP-WebScraper/3.0

# Request timeout (milliseconds)
CRAWL_TIMEOUT=30000

# =============================================================================
# SEARCH PROCESSING
# =============================================================================

# Enable/disable search ranking and deduplication
ENABLE_SEARCH_RANKING=true
ENABLE_SEARCH_DEDUPLICATION=true

# Ranking weights (must sum to 1.0)
RANKING_WEIGHT_BM25=0.4
RANKING_WEIGHT_SEMANTIC=0.3
RANKING_WEIGHT_AUTHORITY=0.2
RANKING_WEIGHT_FRESHNESS=0.1

# BM25 parameters
BM25_K1=1.5
BM25_B=0.75

# Deduplication thresholds (0.0 to 1.0)
DEDUP_THRESHOLD_URL=0.8
DEDUP_THRESHOLD_TITLE=0.75
DEDUP_THRESHOLD_CONTENT=0.7
DEDUP_THRESHOLD_COMBINED=0.6

# Deduplication strategies
DEDUP_URL_NORMALIZATION=true
DEDUP_TITLE_FUZZY=true
DEDUP_CONTENT_SIMHASH=true
DEDUP_DOMAIN_CLUSTERING=true

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# SSRF Protection
SSRF_PROTECTION_ENABLED=true
ALLOWED_PROTOCOLS=http:,https:
MAX_REQUEST_SIZE=104857600
MAX_REQUEST_TIMEOUT=60000
MAX_REDIRECTS=5

# Blocked domains (comma-separated)
BLOCKED_DOMAINS=localhost,127.0.0.1,0.0.0.0,metadata.google.internal,169.254.169.254,metadata.azure.com

# Allowed domains (comma-separated, empty = allow all)
ALLOWED_DOMAINS=

# Input validation
INPUT_VALIDATION_ENABLED=true
MAX_STRING_LENGTH=10000
MAX_ARRAY_LENGTH=1000
MAX_OBJECT_DEPTH=10
MAX_REGEX_LENGTH=500
STRICT_VALIDATION_MODE=false

# API Security
REQUIRE_AUTHENTICATION=false
API_KEY_HEADER=X-API-Key
API_KEY=
RATE_LIMIT_BY_KEY=false
AUDIT_LOGGING=true

# Content security
SANITIZE_HTML=true
ALLOWED_HTML_TAGS=p,br,strong,em,u,h1,h2,h3,h4,h5,h6
BLOCK_SCRIPTS=true
BLOCK_IFRAMES=true

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server port (for HTTP mode)
PORT=3000

# Enable security headers
ENABLE_SECURITY_HEADERS=true

# =============================================================================
# OPTIONAL: REDIS CACHE
# =============================================================================

# Redis configuration (if using Redis as cache backend)
# REDIS_URL=redis://localhost:6379
# REDIS_PASSWORD=
# REDIS_DB=0

# =============================================================================
# OPTIONAL: MONITORING
# =============================================================================

# Grafana password (for monitoring stack)
# GRAFANA_PASSWORD=admin

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Development-specific overrides (only used when NODE_ENV=development)

# Reduced limits for development
# MAX_WORKERS=2
# QUEUE_CONCURRENCY=2
# MAX_CRAWL_DEPTH=2
# MAX_PAGES_PER_CRAWL=10

# More verbose logging
# LOG_LEVEL=debug

# Disable caching for testing
# CACHE_ENABLE_DISK=false