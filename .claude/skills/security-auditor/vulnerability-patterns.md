# Vulnerability Patterns

## Critical Vulnerabilities

### SSRF - Server-Side Request Forgery

```javascript
// BAD: Direct URL usage without validation
const response = await fetch(userProvidedUrl);

// GOOD: Validate and sanitize URL
const url = validateAndSanitizeUrl(userProvidedUrl);
if (isAllowedDomain(url)) {
  const response = await fetch(url);
}
```

### Command Injection

```javascript
// BAD: Direct command execution
exec(`curl ${url}`);

// GOOD: Use libraries, avoid shell commands
const response = await fetch(url);
```

### Path Traversal

```javascript
// BAD: Direct file path usage
fs.readFile(userPath);

// GOOD: Validate and sanitize paths
const safePath = path.normalize(userPath);
if (safePath.startsWith(allowedDir)) {
  fs.readFile(safePath);
}
```

## Security Headers

```javascript
// Required security headers
{
  'X-Content-Type-Options': 'nosniff',
  'X-Frame-Options': 'DENY',
  'X-XSS-Protection': '1; mode=block',
  'Strict-Transport-Security': 'max-age=31536000',
  'Content-Security-Policy': "default-src 'self'"
}
```

## Rate Limiting Configuration

```javascript
{
  global: {
    requests_per_minute: 60,
    burst_limit: 10,
    concurrent_requests: 5
  },
  per_domain: {
    default_delay_ms: 1000,
    max_requests_per_minute: 30,
    respect_crawl_delay: true
  }
}
```

## Robots.txt Implementation

```javascript
// Check before crawling
async function canCrawl(url) {
  const robotsRules = await getRobotsRules(url);
  return robotsRules.isAllowed(url, 'CrawlForge');
}
```

## Environment Variables

```bash
# Required security environment variables
MCP_API_KEY=<encrypted>
SEARCH_API_KEY=<encrypted>
RATE_LIMIT_REDIS_URL=<connection-string>
ALLOWED_DOMAINS=<comma-separated-list>
MAX_CRAWL_DEPTH=5
ENABLE_SECURITY_AUDIT=true
```
